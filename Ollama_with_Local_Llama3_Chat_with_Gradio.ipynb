{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM97vVLUeeKznMJxRvkTvVE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgpatel/deeplearning/blob/main/Ollama_with_Local_Llama3_Chat_with_Gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall ollama"
      ],
      "metadata": {
        "id": "EUBtmdVILhPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-xterm"
      ],
      "metadata": {
        "id": "THUW_Zi0Lrar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext colabxterm\n",
        "%xterm\n",
        "\n",
        "#After opening terminal below command need to be executed to run llama3 in ollama\n",
        "#curl https://ollama.ai/install.sh | sh\n",
        "#ollama serve & ollama run llama2"
      ],
      "metadata": {
        "id": "zjFJYK86M10S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%xterm"
      ],
      "metadata": {
        "id": "y6WXhwSYg-r9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OMFI5pYJ-4J"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-core\n",
        "!pip install langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Ollama\n",
        "llm = Ollama(model='llama3')\n",
        "\n",
        "llm.invoke('Write a poem about mother?')\n"
      ],
      "metadata": {
        "id": "BbgPyYjFS7jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LangChain supports many other chat models. Here, we're using Ollama\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# supports many more optional parameters. Hover on your `ChatOllama(...)`\n",
        "# class to view the latest available supported parameters\n",
        "llm = ChatOllama(model=\"llama3\")\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
        "\n",
        "# using LangChain Expressive Language chain syntax\n",
        "# learn more about the LCEL on\n",
        "# /docs/expression_language/why\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# for brevity, response is printed in terminal\n",
        "# You can use LangServe to deploy your application for\n",
        "# production\n",
        "print(chain.invoke({\"topic\": \"Space travel\"}))"
      ],
      "metadata": {
        "id": "x--3sIrqb7jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"Generate an image of {topic}\")\n",
        "\n",
        "# using LangChain Expressive Language chain syntax\n",
        "# learn more about the LCEL on\n",
        "# /docs/expression_language/why\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# for brevity, response is printed in terminal\n",
        "# You can use LangServe to deploy your application for\n",
        "# production\n",
        "print(chain.invoke({\"topic\": \"Space travel\"}))"
      ],
      "metadata": {
        "id": "XzwqFYKKeOAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "FmEjzfiJhxZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, json\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "model = 'llama3' #You can replace the model name if needed\n",
        "context = []\n",
        "\n",
        "\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "#Call Ollama API\n",
        "def generate(prompt, context, top_k, top_p, temp):\n",
        "    r = requests.post('http://localhost:11434/api/generate',\n",
        "                     json={\n",
        "                         'model': model,\n",
        "                         'prompt': prompt,\n",
        "                         'context': context,\n",
        "                         'options':{\n",
        "                             'top_k': top_k,\n",
        "                             'temperature':top_p,\n",
        "                             'top_p': temp\n",
        "                         }\n",
        "                     },\n",
        "                     stream=False)\n",
        "    r.raise_for_status()\n",
        "\n",
        "\n",
        "    response = \"\"\n",
        "\n",
        "    for line in r.iter_lines():\n",
        "        body = json.loads(line)\n",
        "        response_part = body.get('response', '')\n",
        "        print(response_part)\n",
        "        if 'error' in body:\n",
        "            raise Exception(body['error'])\n",
        "\n",
        "        response += response_part\n",
        "\n",
        "        if body.get('done', False):\n",
        "            context = body.get('context', [])\n",
        "            return response, context\n",
        "\n",
        "\n",
        "\n",
        "def chat(input, chat_history, top_k, top_p, temp):\n",
        "\n",
        "    chat_history = chat_history or []\n",
        "\n",
        "    global context\n",
        "    output, context = generate(input, context, top_k, top_p, temp)\n",
        "\n",
        "    chat_history.append((input, output))\n",
        "\n",
        "    return chat_history, chat_history\n",
        "  #the first history in return history, history is meant to update the\n",
        "  #chatbot widget, and the second history is meant to update the state\n",
        "  #(which is used to maintain conversation history across interactions)\n",
        "\n",
        "\n",
        "#########################Gradio Code##########################\n",
        "block = gr.Blocks()\n",
        "\n",
        "\n",
        "with block:\n",
        "\n",
        "    gr.Markdown(\"\"\"<h1><center> Jarvis AI Artist</center></h1>\n",
        "    \"\"\")\n",
        "\n",
        "    chatbot = gr.Chatbot()\n",
        "    message = gr.Textbox(placeholder=\"Type here to ask Jarvis AI Artist\")\n",
        "\n",
        "    state = gr.State()\n",
        "    with gr.Row():\n",
        "        top_k = gr.Slider(0.0,100.0, label=\"top_k\", value=40, info=\"Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)\")\n",
        "        top_p = gr.Slider(0.0,1.0, label=\"top_p\", value=0.9, info=\" Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)\")\n",
        "        temp = gr.Slider(0.0,2.0, label=\"temperature\", value=0.8, info=\"The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)\")\n",
        "\n",
        "\n",
        "    submit = gr.Button(\"SEND\")\n",
        "\n",
        "    submit.click(chat, inputs=[message, state, top_k, top_p, temp], outputs=[chatbot, state])\n",
        "\n",
        "\n",
        "block.launch(debug=True)"
      ],
      "metadata": {
        "id": "LWdsTV0jiw_H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}